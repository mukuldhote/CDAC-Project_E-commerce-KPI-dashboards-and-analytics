{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24bc5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b41c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL \").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6128ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import surprise\n",
    "from surprise.model_selection import GridSearchCV, KFold\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "feec884b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed: 17.6min finished\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess data and build the recommendation model\n",
    "def preprocess_and_build_model(df):\n",
    "    # Convert 'product_id' column to string type\n",
    "    df['product_id'] = df['product_id'].astype(str)\n",
    "\n",
    "    # Function to convert product ID to generic alphabetic product name based on category\n",
    "    def convert_product_name(row):\n",
    "        category = row['product_category_name_english']\n",
    "        product_id = row['product_id']  # No need to convert to string now\n",
    "        # Create a generic product name by concatenating category abbreviation and a numerical index\n",
    "        category_abbreviation = ''.join(word[:].upper() for word in category.split())\n",
    "        index = int(''.join(filter(str.isdigit, product_id)))  # Extract numerical part from product_id\n",
    "        product_name = f'{category_abbreviation}_{index}'\n",
    "        return product_name\n",
    "\n",
    "    # Convert product IDs to product names\n",
    "    df['product_name'] = df.apply(convert_product_name, axis=1)\n",
    "\n",
    "    # Preprocess the data\n",
    "    lowest_rating = df['review_score'].min()\n",
    "    highest_rating = df['review_score'].max()\n",
    "\n",
    "    # Create a dictionary to map unique customer IDs to sequential numbers\n",
    "    id_mapping = {id_: idx + 1 for idx, id_ in enumerate(df['customer_id'].unique())}\n",
    "\n",
    "    # Convert customer IDs to simple generic unique numbers\n",
    "    df['customer_id'] = df['customer_id'].map(id_mapping)\n",
    "\n",
    "    # Define the Reader\n",
    "    reader = surprise.Reader(rating_scale=(int(lowest_rating), int(highest_rating)))\n",
    "    data = surprise.Dataset.load_from_df(df[['customer_id', 'product_name', 'review_score']], reader)\n",
    "\n",
    "    # Grid search for best parameters\n",
    "    param_grid = {'lr_all': np.linspace(0.001, 1, 3), 'reg_all': np.linspace(0.01, 0.8, 3),\n",
    "                  'n_factors': [40, 30]}\n",
    "    kfold = KFold(random_state=23, n_splits=5, shuffle=True)\n",
    "    gs = GridSearchCV(surprise.SVD, param_grid, joblib_verbose=3, measures=['rmse', 'mae'], cv=kfold, n_jobs=-1)\n",
    "    gs.fit(data)\n",
    "\n",
    "    algo = gs.best_estimator['rmse']\n",
    "    algo.fit(data.build_full_trainset())\n",
    "\n",
    "    # Dumping the model using pickle\n",
    "    with open('recommendation_model.pkl', 'wb') as model_file:\n",
    "        pickle.dump(algo, model_file)\n",
    "\n",
    "    # Dumping id_mapping for later use\n",
    "    with open('id_mapping.pkl', 'wb') as id_mapping_file:\n",
    "        pickle.dump(id_mapping, id_mapping_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = spark.table(\"ecomm.olist_data_table_s\")\\\n",
    "    .select('customer_id','product_id', 'product_category_name_english', 'review_score').toPandas()\n",
    "    preprocess_and_build_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfe2d22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
